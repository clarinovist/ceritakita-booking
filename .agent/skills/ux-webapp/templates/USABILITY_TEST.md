# Usability Test Plan

## Overview

| Field | Value |
|-------|-------|
| Feature/Flow being tested | |
| Test type | Moderated / Unmoderated |
| Duration | 30-45 min per session |
| Participant count | 5 |
| Date range | |

---

## Objectives

### Primary Research Question
_What decision will this test inform?_

### Hypotheses
1. We believe that... because...
2. We believe that... because...

### What We're NOT Testing
_Scope boundaries:_

---

## Participants

### Recruiting Criteria
**Must have:**
-
-

**Nice to have:**
-
-

**Exclude:**
-
-

### Participant Mix
| Segment | Count | Notes |
|---------|-------|-------|
| | | |
| | | |

---

## Setup

### Environment
- [ ] Device/browser:
- [ ] Prototype or staging URL:
- [ ] Test account credentials:
- [ ] Test data seeded:

### Recording
- [ ] Screen recording:
- [ ] Audio recording:
- [ ] Consent form signed:

### Materials
- [ ] Task prompts printed/accessible
- [ ] Note-taking template ready
- [ ] Observer access set up

---

## Script

### Introduction (5 min)
> "Hi, thanks for joining. I'm [name]. Today we're testing [product/feature], not testing youâ€”there are no wrong answers. I'll ask you to complete some tasks and think aloud as you go. Feel free to share any thoughts, positive or negative. Do you have any questions before we start?"

**Consent reminder:**
> "We'll be recording this session. The recording will only be used by our team to improve the product. Is that okay with you?"

### Warm-up Questions (3-5 min)
1. "Tell me a bit about yourself and how you typically [relevant context]."
2. "Have you used [similar product/feature] before? How was that experience?"
3. "What would you expect from a tool that [does X]?"

---

## Tasks

### Task 1: [Task Name]
**Scenario prompt (read aloud):**
> "[Realistic scenario that sets context without leading the user]"

**Starting point:** [URL or screen]

**Success criteria:**
- [ ] User completes [action] without assistance
- [ ] User [specific observable outcome]
- [ ] Time under [X] minutes

**Follow-up questions:**
- "What did you expect to happen when you clicked that?"
- "Was anything confusing or unclear?"
- "On a scale of 1-5, how confident were you that you completed this correctly?"

---

### Task 2: [Task Name]
**Scenario prompt:**
> "[Scenario]"

**Starting point:** [URL or screen]

**Success criteria:**
- [ ]
- [ ]

**Follow-up questions:**
-
-

---

### Task 3: [Task Name]
**Scenario prompt:**
> "[Scenario]"

**Starting point:** [URL or screen]

**Success criteria:**
- [ ]
- [ ]

**Follow-up questions:**
-
-

---

### Task 4: [Task Name] (optional)
**Scenario prompt:**
> "[Scenario]"

**Starting point:** [URL or screen]

**Success criteria:**
- [ ]
- [ ]

**Follow-up questions:**
-
-

---

## Post-Task Rating

After each task, ask:

> "On a scale of 1 to 5, where 1 is very difficult and 5 is very easy, how would you rate that task?"

| Task | Difficulty (1-5) | Confidence (1-5) |
|------|------------------|------------------|
| Task 1 | | |
| Task 2 | | |
| Task 3 | | |
| Task 4 | | |

---

## Debrief Questions (5-10 min)

1. "Overall, what was your impression of [product/feature]?"
2. "What was the easiest part of what you did today?"
3. "What was the most frustrating or confusing part?"
4. "If you could change one thing, what would it be?"
5. "Is there anything else you'd like to share?"

---

## Metrics

### Quantitative
| Metric | Task 1 | Task 2 | Task 3 | Task 4 |
|--------|--------|--------|--------|--------|
| Success (Y/N) | | | | |
| Time on task | | | | |
| Errors (count) | | | | |
| Assists needed | | | | |
| Difficulty rating | | | | |

### Qualitative
- Key observations:
- Notable quotes:
- Pain points:
- Positive moments:

---

## Note-Taking Template

Use during each session:

```
Participant: P[#]
Date/Time:

Task 1:
- Success: Y / N / Partial
- Time:
- Errors:
- Observations:
- Quotes:

Task 2:
...

Overall impressions:

Key takeaway:
```

---

## Analysis Framework

### After All Sessions

1. **Success rates by task**
   - Which tasks had lowest success? Why?

2. **Common pain points**
   - What issues appeared across multiple participants?

3. **Severity assessment**
   - Critical (blocks completion):
   - Major (significant friction):
   - Minor (small improvements):

4. **Recommendations**
   - Must fix:
   - Should fix:
   - Could fix:

---

## Reporting Template

### Executive Summary
_2-3 sentences on key findings and recommended actions_

### Methodology
_Brief description of approach_

### Key Findings
1. Finding + evidence + recommendation
2. Finding + evidence + recommendation
3. Finding + evidence + recommendation

### Detailed Results
_Per-task breakdown with success rates, times, and observations_

### Recommendations (Prioritized)
| Priority | Issue | Recommendation | Effort |
|----------|-------|----------------|--------|
| 1 | | | |
| 2 | | | |
| 3 | | | |

### Next Steps
- [ ]
- [ ]
